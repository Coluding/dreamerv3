{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecf3bbf",
   "metadata": {},
   "source": [
    "# Experiments Notebook\n",
    "This notebook contains the calls needed to replicate the experiments run in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b872c6e8",
   "metadata": {},
   "source": [
    "### Reproduction\n",
    "1. Select the Benchmark and tasks by defining a set containing the benchmark (only one at a time) and a set containing all tasks to run. For example: \n",
    "    ```\n",
    "    DEFAULT_DATASETS = {\"atari100k\"}\n",
    "    ATARI_TASKS = {\"atari100k_krull\", \"atari100k_battle_zone\", \"atari100k_boxing\"}\n",
    "    ```\n",
    "    Pass them to the `run_experiment` function for `datasets` and `tasks` respectively.\n",
    "2. The configurations defined in `presets.py` will override the `configs.yaml`. Make sure they are as desired.\n",
    "3. Run the experiment using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583df0ff",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_standard_dreamer --name \"DreamerV3 Baseline\" --description \"DreamerV3 standard configuration run\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3257a9",
   "metadata": {},
   "source": [
    "### Optimized Replay Buffer\n",
    "Follow steps 1-3 from the previous section. To activate the prioritized replay buffer, `replay_context` has to be 1. The remaining important configurations we used are listed below:\n",
    "* `\"replay.fracs.uniform\"`: `0.0`\n",
    "* `\"replay.fracs.priority\"`: `1.0`\n",
    "* `\"replay.fracs.recency\"`: `0.0`\n",
    "* `\"replay.prio.exponent\"`: `0.8`\n",
    "* `\"replay.prio.maxfrac\"`: `0.5`\n",
    "* `\"replay.prio.initial\"`: `1.0`\n",
    "* `\"replay.prio.zero_on_sample\"`: `False`\n",
    "\n",
    "<br><br>\n",
    "When setup, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d934d4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_replay_buffer_experiment --name \"DreamerV3 Prioritized Replay Buffer\" --description \"DreamerV3 optimized replay buffer configuration run\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c69fd",
   "metadata": {},
   "source": [
    "### Latent Reward Disagreement (Exp. Decay)\n",
    "Follow steps 1-3 from the previous section. To activate the latent reward disagreement, set `agent.use_intrinsic` to `True` and `agent.intrinsic.scheduling_strategy` to `\"exp_decay\"` for exponential decay scheduling. The remaining important configurations we used for our experiments are listed below:\n",
    "* `\"agent.intrinsic.learn_strategy\"`: `\"joint_mlp\"` > Other options are ema and perturbed_starts\n",
    "* `\"agent.intrinsic.exploration_type\"`: `\"reward_variance\"` > Other options are state_disagreement\n",
    "* `\"agent.intrinsic.reward_type\"`: `\"disagreement\"` > Other options include prediction_error and max_disagreement\n",
    "* `\"agent.intrinsic.scheduling_strategy\"`: `\"exp_decay\"`\n",
    "\n",
    "<br><br>\n",
    "When setup, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fa2d7e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_latent_disagreement_experiment_exp_decay --name \"DreamerV3 Latent Reward Disagreement with exponential decay scheduling\" --description \"DreamerV3 guided by latent reward disagreement with exponential decay scheduling\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee935f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Latent Reward Disagreement (Exponential Moving Average Slope)\n",
    "Follow steps 1-3 from the previous section. To activate the latent reward disagreement, set `agent.use_intrinsic` to `True` and `agent.intrinsic.scheduling_strategy` to `\"slope_ema\"` for EMA Slope scheduling. The remaining important configurations we used for our experiments are listed below:\n",
    "* `\"agent.intrinsic.learn_strategy\"`: `\"joint_mlp\"` > Other options are ema and perturbed_starts\n",
    "* `\"agent.intrinsic.exploration_type\"`: `\"reward_variance\"` > Other options are state_disagreement\n",
    "* `\"agent.intrinsic.reward_type\"`: `\"disagreement\"` > Other options include prediction_error and max_disagreement\n",
    "* `\"agent.intrinsic.scheduling_strategy\"`: `\"slope_ema\"`\n",
    "\n",
    "<br><br>\n",
    "When setup, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6316c45",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_latent_disagreement_experiment_ema --name \"DreamerV3 Latent Reward Disagreement with EMA slope scheduling\" --description \"DreamerV3 guided by latent reward disagreement with EMA slope scheduling\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9dc9f0",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results are logged in the logdir. For plotting the results, please refer to the readme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65142f0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Individual Contributions\n",
    "Most of the ideation behind our extensions was conducted in brainstorming sessions that all team members attended. Even if it was not their main contribution, all team member contributed to all parts of this work. Parts of the implementation were done in peer-coding sessions.\n",
    "- Lukas Bierling: Major efforts on the implementation of all extensions and its variants. Collaboration on the ideation and interpretation of results. Coordinated workstreams and repository use. General collaboration on ideation & implementation as indicated above.\n",
    "- Davide Paserio: Design & parts of the Prioritized Replay Buffer implementation, contribution to the implementation of the latent reward disagreement. General collaboration on ideation & implementation as indicated above.\n",
    "- Jan Henrik Bertrand: Desgin & parts of the Latent Reward Disagreement implementation. Design & Implementation of the experimental framework. Running the experiments. General collaboration on ideation & implementation as indicated above.\n",
    "- Kiki van Gerwen: Design of the custom plotting tool, contribution to the implementation of the latent reward disagreement. General collaboration on ideation & implementation as indicated above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
