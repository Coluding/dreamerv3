{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecf3bbf",
   "metadata": {},
   "source": [
    "# DreamerV3-XP\n",
    "This notebook contains the calls needed to replicate the experiments run in this project, visualizations and deeper technical explanations about our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1427d",
   "metadata": {},
   "source": [
    "### Reproduction\n",
    "1. Select the Benchmark and tasks by defining a set containing the benchmark (only one at a time) and a set containing all tasks to run. For example: \n",
    "    ```\n",
    "    DEFAULT_DATASETS = {\"atari100k\"}\n",
    "    ATARI_TASKS = {\"atari100k_krull\", \"atari100k_battle_zone\", \"atari100k_boxing\"}\n",
    "    ```\n",
    "    Pass them to the `run_experiment` function for `datasets` and `tasks` respectively.\n",
    "2. The configurations defined in `presets.py` will override the `configs.yaml`. Make sure they are as desired.\n",
    "3. Run the experiment using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e07ab7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_standard_dreamer --name \"DreamerV3 Baseline\" --description \"DreamerV3 standard configuration run\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7453c",
   "metadata": {},
   "source": [
    "### Optimized Replay Buffer\n",
    "Follow steps 1-3 from the previous section. To activate the prioritized replay buffer, `replay_context` has to be 1. The remaining important configurations we used are listed below:\n",
    "* `\"replay.fracs.uniform\"`: `0.0`\n",
    "* `\"replay.fracs.priority\"`: `1.0`\n",
    "* `\"replay.fracs.recency\"`: `0.0`\n",
    "* `\"replay.prio.exponent\"`: `0.8`\n",
    "* `\"replay.prio.maxfrac\"`: `0.5`\n",
    "* `\"replay.prio.initial\"`: `1.0`\n",
    "* `\"replay.prio.zero_on_sample\"`: `False`\n",
    "\n",
    "<br><br>\n",
    "When setup, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9db820",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_replay_buffer_experiment --name \"DreamerV3 Prioritized Replay Buffer\" --description \"DreamerV3 optimized replay buffer configuration run\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55328f35",
   "metadata": {},
   "source": [
    "### Latent Reward Disagreement (Exp. Decay)\n",
    "Follow steps 1-3 from the previous section. To activate the latent reward disagreement, set `agent.use_intrinsic` to `True` and `agent.intrinsic.scheduling_strategy` to `\"exp_decay\"` for exponential decay scheduling. The remaining important configurations we used for our experiments are listed below:\n",
    "* `\"agent.intrinsic.learn_strategy\"`: `\"joint_mlp\"` > Other options are ema and perturbed_starts\n",
    "* `\"agent.intrinsic.exploration_type\"`: `\"reward_variance\"` > Other options are state_disagreement\n",
    "* `\"agent.intrinsic.reward_type\"`: `\"disagreement\"` > Other options include prediction_error and max_disagreement\n",
    "* `\"agent.intrinsic.scheduling_strategy\"`: `\"exp_decay\"`\n",
    "\n",
    "<br><br>\n",
    "When setup, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c169048",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_latent_disagreement_experiment_exp_decay --name \"DreamerV3 Latent Reward Disagreement with exponential decay scheduling\" --description \"DreamerV3 guided by latent reward disagreement with exponential decay scheduling\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13769fb1",
   "metadata": {},
   "source": [
    "### Latent Reward Disagreement (Exponential Moving Average Slope)\n",
    "Follow steps 1-3 from the previous section. To activate the latent reward disagreement, set `agent.use_intrinsic` to `True` and `agent.intrinsic.scheduling_strategy` to `\"slope_ema\"` for EMA Slope scheduling. The remaining important configurations we used for our experiments are listed below:\n",
    "* `\"agent.intrinsic.learn_strategy\"`: `\"joint_mlp\"` > Other options are ema and perturbed_starts\n",
    "* `\"agent.intrinsic.exploration_type\"`: `\"reward_variance\"` > Other options are state_disagreement\n",
    "* `\"agent.intrinsic.reward_type\"`: `\"disagreement\"` > Other options include prediction_error and max_disagreement\n",
    "* `\"agent.intrinsic.scheduling_strategy\"`: `\"slope_ema\"`\n",
    "\n",
    "<br><br>\n",
    "When setup, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382d9f0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python experiments/experiment_definitions.py run_latent_disagreement_experiment_ema --name \"DreamerV3 Latent Reward Disagreement with EMA slope scheduling\" --description \"DreamerV3 guided by latent reward disagreement with EMA slope scheduling\" --num_seeds 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a43729",
   "metadata": {},
   "source": [
    "### Results\n",
    "The results are logged in the logdir. For plotting the results, please refer to the readme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4323a7",
   "metadata": {},
   "source": [
    "## Individual Contributions\n",
    "Most of the ideation behind our extensions was conducted in brainstorming sessions that all team members attended. Even if it was not their main contribution, all team member contributed to all parts of this work. Parts of the implementation were done in peer-coding sessions.\n",
    "- Lukas Bierling: Major efforts on the implementation of all extensions and its variants. Collaboration on the ideation and interpretation of results. Coordinated workstreams and repository use. General collaboration on ideation & implementation as indicated above.\n",
    "- ⁠Davide Paserio: Design & parts of the Prioritized Replay Buffer implementation, contribution to the implementation of the latent reward disagreement. General collaboration on ideation & implementation as indicated above.\n",
    "- Jan Henrik Bertrand: Desgin & parts of the Latent Reward Disagreement implementation. Design & Implementation of the experimental framework. Running the experiments. General collaboration on ideation & implementation as indicated above.\n",
    "- ⁠Kiki van Gerwen: Design of the custom plotting tool, contribution to the implementation of the latent reward disagreement. General collaboration on ideation & implementation as indicated above. \n",
    " The code was committed from the Snellius system under Jan's GitHub account due to shared access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfbe448",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80489173",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "import io\n",
    "import matplotlib.animation as animation\n",
    "from scipy.special import expit as sigmoid\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d349c7",
   "metadata": {},
   "source": [
    "# Latent reward disagreement\n",
    "Inspired by Plan2Explore's \\cite{sekar2020plan2explore} \"disagreement\" over latent states predicted by an ensemble of world models, we use the disagreement over reward predictions from an ensemble of world models. To quantify the disagreement, the variance over the predicted rewards is taken and added to the mean of the predicted rewards to incentivize trajectories that are expected to be rewarding. This sum of mean and variance is our intrinsic reward. \n",
    "Each ensemble member $k \\in \\{1, .., K\\}$, parameterized by $w_k$, recurrently predicts (i.e., \"imagines\") future deterministic latent states $h_{t'}^{w_k}$ over imagination horizon $L$ with $t'$ being a timestep within the horizon. The standard reward predictor then predicts the corresponding reward $\\hat{r}_{k, t'} \\sim p_\\phi(\\hat{r}_{t'} | h_{t'}^{w_k}, z_{t'})$. Formally,\n",
    "$$\n",
    "r_{t}^{intr} = \\frac{1}{L} \\sum_{t'=t}^{t+L} \\left[ \\bar{r}_{t'} + \\frac{1}{K} \\sum_{k=1}^{K} (\\hat{r}_{k,t'} - \\bar{r}_{t'})^2 \\right]\n",
    "$$\n",
    "where $\\bar{r}_{t'}$ is the mean predicted reward across all ensemble members at timestep $t'$ of the imagination. High variance indicates epistemic uncertainty over the predicted reward, and thus encourages exploration of the associated state. The final reward used for training is a convex combination of extrinsic and intrinsic rewards: \n",
    "$$r_t^{\\text{total}} = \\lambda r_t^{\\text{ext}} + (1-\\lambda) r_t^{\\text{intr}}$$\n",
    "\n",
    "The following code produces a simple visualization that explains the advantages of having intrinsic motivation alongside the extrinsic reward compared to having only environmental rewards in sparse rewards settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba058a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maze settings\n",
    "maze_size = (7, 7)\n",
    "goal_pos = (6, 6)\n",
    "start_pos = (0, 0)\n",
    "num_steps = 30\n",
    "\n",
    "# Intrinsic reward settings\n",
    "intr_reward_low, intr_reward_high = 0.08, 0.13\n",
    "\n",
    "def run_agent(record_intrinsic):\n",
    "    pos = list(start_pos)\n",
    "    trajectory = [tuple(pos)]\n",
    "    rewards = []\n",
    "    intrinsic_rewards = []\n",
    "    visited = set()\n",
    "    for step in range(num_steps):\n",
    "        visited.add(tuple(pos))\n",
    "        # Random valid move\n",
    "        moves = []\n",
    "        for dx, dy in [(-1,0),(1,0),(0,-1),(0,1)]:\n",
    "            nx, ny = pos[0]+dx, pos[1]+dy\n",
    "            if 0 <= nx < maze_size[0] and 0 <= ny < maze_size[1]:\n",
    "                moves.append((nx, ny))\n",
    "        if moves:\n",
    "            if record_intrinsic:\n",
    "                unvisited = [m for m in moves if m not in visited]\n",
    "                if unvisited:\n",
    "                    pos = list(unvisited[np.random.randint(len(unvisited))])\n",
    "                else:\n",
    "                    pos = list(moves[np.random.randint(len(moves))])\n",
    "            else:\n",
    "                pos = list(moves[np.random.randint(len(moves))])\n",
    "        trajectory.append(tuple(pos))\n",
    "        # Reward logic\n",
    "        extrinsic = 1 if tuple(pos) == goal_pos else 0\n",
    "        if record_intrinsic:\n",
    "            if extrinsic:\n",
    "                intr = 0\n",
    "            elif tuple(pos) not in trajectory[:-1]:  # novel state\n",
    "                intr = np.random.uniform(intr_reward_low, intr_reward_high)\n",
    "            else:\n",
    "                intr = 0\n",
    "            rewards.append(extrinsic + intr)\n",
    "            intrinsic_rewards.append(intr)\n",
    "        else:\n",
    "            rewards.append(extrinsic)\n",
    "            intrinsic_rewards.append(0)\n",
    "    return trajectory, rewards, intrinsic_rewards\n",
    "\n",
    "extrinsic_traj, extrinsic_rewards, _ = run_agent(record_intrinsic=False)\n",
    "intrinsic_traj, intrinsic_total_rewards, intrinsic_rewards = run_agent(record_intrinsic=True)\n",
    "\n",
    "# Generate GIF frames\n",
    "frames = []\n",
    "extrinsic_return = 0\n",
    "intrinsic_return = 0\n",
    "\n",
    "for t in range(num_steps+1):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(9,4.5))\n",
    "    for idx, (ax, traj, rewards, intrinsic, label) in enumerate(\n",
    "        zip(\n",
    "            axs,\n",
    "            [extrinsic_traj, intrinsic_traj],\n",
    "            [extrinsic_rewards, intrinsic_total_rewards],\n",
    "            [_, intrinsic_rewards],\n",
    "            ['Sparse (Extrinsic) Reward', 'Intrinsic + Extrinsic Reward']\n",
    "        )\n",
    "    ):\n",
    "        ax.set_title(label, fontsize=11)\n",
    "        ax.set_xlim(-0.5, maze_size[0]-0.5)\n",
    "        ax.set_ylim(-0.5, maze_size[1]-0.5)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # Draw grid\n",
    "        for i in range(maze_size[0]+1):\n",
    "            ax.plot([i-0.5, i-0.5], [-0.5, maze_size[1]-0.5], color='gray', linewidth=0.5)\n",
    "            ax.plot([-0.5, maze_size[0]-0.5], [i-0.5, i-0.5], color='gray', linewidth=0.5)\n",
    "\n",
    "        # Draw goal\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (goal_pos[0]-0.5, goal_pos[1]-0.5), 1, 1, color='gold', alpha=0.7, zorder=0\n",
    "        ))\n",
    "        ax.text(goal_pos[0], goal_pos[1], \"Goal\", ha='center', va='center', fontsize=8, color='black')\n",
    "\n",
    "        # Draw agent's trail and rewards\n",
    "        for i, (x, y) in enumerate(traj[:t+1]):\n",
    "            if i == 0:\n",
    "                color = 'gray'\n",
    "                intrinsic_val = 0\n",
    "            else:\n",
    "                color = (\n",
    "                    'royalblue' if label.startswith('Intrinsic') and intrinsic[i-1] > 0\n",
    "                    else ('red' if i != len(traj)-1 else 'green')\n",
    "                )\n",
    "                intrinsic_val = intrinsic[i-1]\n",
    "\n",
    "            # Trail\n",
    "            ax.plot(x, y, 'o', color=color, markersize=7, alpha=0.7 if i < t else 1.0, zorder=1)\n",
    "            # Reward marker\n",
    "            if (x, y) == goal_pos:\n",
    "                ax.text(x, y+0.3, \"+1\", ha='center', va='bottom', color='green', fontsize=10, fontweight='bold')\n",
    "            elif label.startswith('Intrinsic') and i > 0 and intrinsic_val > 0:\n",
    "                ax.text(x, y+0.2, f\"+{intrinsic_val:.2f}\", ha='center', va='bottom', color='blue', fontsize=8, fontweight='bold')\n",
    "            elif label.startswith('Sparse') and i > 0 and (x, y) != goal_pos:\n",
    "                ax.text(x, y+0.2, \"0\", ha='center', va='bottom', color='gray', fontsize=8, fontweight='bold')\n",
    "\n",
    "        # Draw robot agent (last)\n",
    "        x, y = traj[t]\n",
    "        ax.text(x, y, \"🤖\", fontsize=23, ha='center', va='center', zorder=2)\n",
    "\n",
    "        # Show reward at current step\n",
    "        if t > 0:\n",
    "            if (x, y) == goal_pos:\n",
    "                ax.text(x, y+0.6, \"+1\", ha='center', va='bottom', color='green', fontsize=13, fontweight='bold')\n",
    "            elif label.startswith('Intrinsic') and intrinsic[t-1] > 0:\n",
    "                ax.text(x, y+0.6, f\"+{intrinsic[t-1]:.2f}\", ha='center', va='bottom', color='blue', fontsize=12, fontweight='bold')\n",
    "            elif label.startswith('Sparse') and (x, y) != goal_pos:\n",
    "                ax.text(x, y+0.6, \"0\", ha='center', va='bottom', color='gray', fontsize=12, fontweight='bold')\n",
    "\n",
    "        # Compute return so far\n",
    "        curr_return = np.sum(rewards[:t+1])\n",
    "        if idx == 0:\n",
    "            extrinsic_return = curr_return\n",
    "        else:\n",
    "            intrinsic_return = curr_return\n",
    "\n",
    "        # Draw cumulative return\n",
    "        ax.text(0, -1.2, f\"Return: {curr_return:.2f}\", fontsize=12, ha='left', color='black')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(bottom=0.18)\n",
    "    fig.canvas.draw()\n",
    "    buf = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "    w, h = fig.canvas.get_width_height()\n",
    "    frame = buf.reshape((h, w, 4))[..., :3]\n",
    "    frames.append(frame)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Save GIF\n",
    "# Save the GIF to an in-memory buffer\n",
    "gif_buffer = io.BytesIO()\n",
    "imageio.mimsave(gif_buffer, frames, format='gif', duration=1.01)\n",
    "gif_buffer.seek(0)\n",
    "\n",
    "# Display in Jupyter notebook\n",
    "display(Image(data=gif_buffer.read(), format='gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bd285",
   "metadata": {},
   "source": [
    "## EMA gradient as a scheduler for the importance of the intrinsic reward in the total reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb0c88",
   "metadata": {},
   "source": [
    "To balance exploration and exploitation, we combine extrinsic and intrinsic rewards using a weighting factor $\\lambda$. We experiment with two strategies for adapting $\\lambda$ over time. First, we apply exponential decay, gradually reducing the influence of intrinsic rewards as training progresses. Second, we explore a dynamic adjustment using the gradient of an exponential moving average (EMA) of the episode return: $\\lambda$ is decreased when performance tends to improve and increased when learning stagnates or regresses. This encourages exploration when necessary and promotes exploitation when training is stable.\n",
    "\n",
    "The following code generate a simple visualization to understand how the exponential moving average gradient of the episode return works as a scheduler of the importance of the intrinsic reward on the overall reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be405086",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "steps = 5000\n",
    "\n",
    "# --- 1. Reward pattern: down, up, plateau, down, big up ---\n",
    "segments = [\n",
    "    np.linspace(40_000, 15_000, 600),              # down\n",
    "    np.linspace(15_000, 45_000, 1000),             # up\n",
    "    np.ones(700) * 45_000,                         # plateau\n",
    "    np.linspace(45_000, 20_000, 900),              # down\n",
    "    np.linspace(20_000, 90_000, steps - 3200)      # big up\n",
    "]\n",
    "reward = np.concatenate(segments)\n",
    "reward = reward[:steps]\n",
    "reward += np.random.normal(0, 15_000, size=steps)  # Even more noise\n",
    "reward = np.clip(reward, 0, None)\n",
    "\n",
    "# --- 2. EMA ---\n",
    "def compute_ema(data, alpha=0.01):\n",
    "    ema = np.zeros_like(data)\n",
    "    ema[0] = data[0]\n",
    "    for t in range(1, len(data)):\n",
    "        ema[t] = alpha * data[t] + (1 - alpha) * ema[t - 1]\n",
    "    return ema\n",
    "\n",
    "ema = compute_ema(reward, alpha=0.01)\n",
    "\n",
    "window = 500\n",
    "n_frames = steps - window\n",
    "\n",
    "slopes = np.zeros(n_frames)\n",
    "lambdas = np.zeros(n_frames)\n",
    "\n",
    "for i in range(n_frames):\n",
    "    ema_window = ema[i:i+window]\n",
    "    ema_min, ema_max = ema_window.min(), ema_window.max()\n",
    "    if ema_max - ema_min == 0:\n",
    "        scaled = np.zeros_like(ema_window)\n",
    "    else:\n",
    "        scaled = (ema_window - ema_min) / (ema_max - ema_min)\n",
    "    slope = scaled[-1] - scaled[0]\n",
    "    slopes[i] = slope\n",
    "    lambdas[i] = sigmoid(slope)\n",
    "\n",
    "# --- 3. Animation ---\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 4), sharex=True, gridspec_kw={'height_ratios':[2,1]})\n",
    "\n",
    "n_gif_frames = 60\n",
    "frame_indices = np.linspace(0, n_frames-1, n_gif_frames).astype(int)\n",
    "\n",
    "def animate(frame):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    start = frame\n",
    "    end = frame + window\n",
    "\n",
    "    # Top plot: EMA (scaled), current window\n",
    "    full_scaled = (ema - ema.min()) / (ema.max() - ema.min())\n",
    "    ax1.plot(np.arange(steps), full_scaled, color='blue', alpha=0.3, lw=1, label='EMA of rewards')\n",
    "    window_ema = ema[start:end]\n",
    "    win_min, win_max = window_ema.min(), window_ema.max()\n",
    "    if win_max - win_min == 0:\n",
    "        window_scaled = np.zeros_like(window_ema)\n",
    "    else:\n",
    "        window_scaled = (window_ema - win_min) / (win_max - win_min)\n",
    "    ax1.plot(np.arange(start, end), window_scaled, color='red', lw=1.5, label='Current window')\n",
    "    ax1.scatter([start, end-1], [window_scaled[0], window_scaled[-1]], color='black', zorder=10, s=24)\n",
    "    # Draw slope as orange line\n",
    "    ax1.plot(\n",
    "        [start, end-1],\n",
    "        [window_scaled[0], window_scaled[-1]],\n",
    "        color='orange', lw=2, label='Slope'\n",
    "    )\n",
    "    ax1.set_ylabel('EMA (window min-max scaled)')\n",
    "    ax1.legend(loc='upper left', fontsize=8)\n",
    "    ax1.set_title(f'Window: {start}-{end-1} | Slope: {slopes[frame]:.2f} | Lambda: {lambdas[frame]:.2f}')\n",
    "    ax1.set_ylim(-0.2, 1.2)\n",
    "\n",
    "    # Bottom plot: Lambda\n",
    "    ax2.plot(np.arange(frame+1), lambdas[:frame+1], color='green', lw=1.5, label='Lambda')\n",
    "    ax2.set_xlim(0, n_frames)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_ylabel('Lambda')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.legend(loc='upper left', fontsize=8)\n",
    "    ax2.axvline(frame, color='grey', lw=1, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, animate, frames=frame_indices, interval=300, repeat=False\n",
    ")\n",
    "\n",
    "gif_path = \"lambda_animation.gif\"\n",
    "ani.save(gif_path, writer='pillow', fps=4)\n",
    "plt.close(fig)  # Prevent extra empty plot\n",
    "\n",
    "# Display the GIF in the notebook\n",
    "display(Image(filename=gif_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreamerv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
